{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Frequentist vs Bayesianism part 2\nJake VanderPlas\n\nhttp://jakevdp.github.io/blog/2014/06/06/frequentism-and-bayesianism-2-when-results-differ/"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"try:\n    import numpy as np, pandas as pd, matplotlib.pyplot as plt, emcee\nexcept:\n    ! conda install -y emcee\n    import numpy as np, pandas as pd, matplotlib.pyplot as plt, emcee","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Example 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"def report(method, p):\n    print(f\"{method} Probability of Bob Winning: {p*100:.0f}% or {(1-p)/p:.0f} to 1 against\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"method = \"Na√Øve Frequentist\"\np_hat = 5. / 8.\nfreq_prob = (1 - p_hat) ** 3\nreport(method, freq_prob)\n# print(f\"{method} Probability of Bob Winning: {p:.2f} or {(1-p)/p:.0f} to 1 against\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"method = \"Closed-Form Bayesian\"\nfrom scipy.special import beta\nbayes_prob = beta(6 + 1, 5 + 1) / beta(3 + 1, 5 + 1)\nreport(method, bayes_prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"method = \"Simulation\"\nrng = np.random.default_rng(0)\n\n# play 100000 games with randomly-drawn p, between 0 and 1\np = rng.uniform(0, 1, 100000)\n\n# each game needs at most 11 rolls for one player to reach 6 wins\nrolls = rng.uniform(0, 1, (11, len(p)))\n\n# count the cumulative wins for Alice and Bob at each roll\nAlice_count = np.cumsum(rolls < p, 0)\nBob_count = np.cumsum(rolls >= p, 0)\n\n# sanity check: total number of wins should equal number of rolls\ntotal_wins = Alice_count + Bob_count\nassert np.all(total_wins.T == np.arange(1, 12))\nprint(\"(Sanity check passed)\")\n\n# determine number of games which meet our criterion of (A wins, B wins)=(5, 3)\n# this means Bob's win count at eight rolls must equal 3\ngood_games = Bob_count[7] == 3\nprint(\"Number of suitable games: {0}\".format(good_games.sum()))\n\n# truncate our results to consider only these games\nAlice_count = Alice_count[:, good_games]\nBob_count = Bob_count[:, good_games]\n\n# determine which of these games Bob won.\n# to win, he must reach six wins after 11 rolls.\nbob_won = np.sum(Bob_count[10] == 6)\nprint(\"Number of these games Bob won: {0}\".format(bob_won.sum()))\n\n# compute the probability\nsim_prob = bob_won.sum() * 1. / good_games.sum()\n\nreport(method, sim_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Example 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array([ 0,  3,  9, 14, 15, 19, 20, 21, 30, 35,\n              40, 41, 42, 43, 54, 56, 67, 69, 72, 88])\ny = np.array([33, 68, 34, 34, 37, 71, 37, 44, 48, 49,\n              53, 49, 50, 48, 56, 60, 61, 63, 44, 71])\ne = np.array([ 3.6, 3.9, 2.6, 3.4, 3.8, 3.8, 2.2, 2.1, 2.3, 3.8,\n               2.2, 2.8, 3.9, 3.1, 3.4, 2.6, 3.4, 3.7, 2.0, 3.5])\nplt.errorbar(x, y, e, fmt='.k', ecolor='gray');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import optimize\nfrom functools import partial\n\ndef f(theta, x=x):\n    return theta[0] + theta[1] * x\n\ndef squared_loss(theta):\n    dy = y - f(theta)\n    return (0.5 * (dy / e) ** 2).sum()\n\ndef huber_loss(theta, c=3):\n    dy = y - f(theta)\n    return ((abs(dy) <  c) * (0.5 * dy ** 2) +\n            (abs(dy) >= c) * (0.5 * c - abs(dy)) * -c\n           ).sum()\n\ndef fit(loss):\n    return optimize.fmin(loss, thetas['init'], disp=False)\n\ndef plot_fit(theta, name=None):\n    plt.errorbar(x, y, e, fmt='.k', ecolor='gray')    \n    xx = np.linspace(0, 100)\n    plt.plot(xx, f(theta, xx), label=name)\n    plt.legend()\n\n\nthetas = dict()\nfig, ax = plt.subplots()\n\nnm = 'init'\nt = [y.mean(), 0]\nthetas[nm] = t\nplot_fit(t, nm)\n\nnm = 'L2'\nt = fit(squared_loss)\nthetas[nm] = t\nplot_fit(t, nm)\n\nr = [3, 10, 100]\nfor c in r:\n    nm = f'Huber{c}'\n    t = fit(partial(huber_loss, c=c))\n    thetas[nm] = t\n    plot_fit(t, nm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_prior(theta, sigma_B):\n    ## uniform on [0,1]\n    if (any(theta[2:] < 0) or any(theta[2:] > 1)):\n        return -np.inf  # recall log(0) = -inf\n    else:\n        return 0\n\ndef log_normal(dy, s):\n    return (np.log(2 * np.pi * s**2) + (dy / s)**2) / -2\n\ndef log_likelihood(theta, sigma_B):\n    dy = y - f(theta)\n    g = np.clip(theta[2:], 0, 1)  # g<0 or g>1 leads to NaNs in logarithm\n    logL1 = np.log(g)   + log_normal(dy, e)\n    logL2 = np.log(1-g) + log_normal(dy, sigma_B)\n    return (np.logaddexp(logL1, logL2)).sum()\n\ndef log_posterior(theta, sigma_B):\n    return log_prior(theta, sigma_B) + log_likelihood(theta, sigma_B)\n\n\nndim = 2 + len(x)  # number of parameters in the model\nnwalkers = 50  # number of MCMC walkers\nnburn = 10000  # \"burn-in\" period to let chains stabilize\nnsteps = 15000  # number of MCMC steps to take\n\nrng = np.random.default_rng(0)\ntheta_init  = rng.normal(thetas['L2'], 1.0, (nwalkers, 2))\n# g_init      = rng.uniform(0.0, 1.0, (nwalkers, ndim-2))\ng_init      = rng.normal (0.5, 0.1, (nwalkers, ndim-2))\nparams_init = np.hstack([theta_init, g_init])\n\nsampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, kwargs={'sigma_B':50})\nsampler.run_mcmc(params_init, nsteps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = sampler.chain  # shape = (nwalkers, nsteps, ndim)\nsample = sample[:, nburn:, :].reshape(-1, ndim).T  # throw out first nburn steps to let chain stabilize, then reshape to (ndim, nsamples)\n\nnm = 'Bayes'\nt = sample.mean(axis=1)\nthetas[nm] = t[:2]\ng = t[2:]\noutliers = (g < 0.5)\n\nfig, ax = plt.subplots()\nplot_fit(t, nm)\nplt.plot(x[outliers], y[outliers], 'ro', ms=20, mfc='none', mec=ax.lines[-1].get_color())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(sample[0], sample[1], ',k', alpha=0.1)\nplt.xlabel('intercept')\nplt.ylabel('slope');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nfor nm, theta in thetas.items():\n    plot_fit(theta, nm)\nplt.plot(x[outliers], y[outliers], 'ro', ms=20, mfc='none', mec=ax.lines[-1].get_color())\nplt.title('Maximum Likelihood fit: Bayesian Marginalization');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}