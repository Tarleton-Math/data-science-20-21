{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "math5364_20fl_final_exam_review_guide.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tarleton-Math/data-science-20-21/blob/master/math5364_20fl_final_exam_review_guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXyBjE1VNF3S"
      },
      "source": [
        "# Final Exam Review Guide\n",
        "## Math 5364 - Data Science 1\n",
        "## Fall 2020 - Dr. Scott Cook - Tarleton State Univ\n",
        "## Version 2020-11-17\n",
        "\n",
        "- Data Science big ideas & definitions: 2020-08-25\n",
        "- Linear Algebra Definitions: 2020-08-27\n",
        "- Performance Metrics: 2020-10-13\n",
        "- Cross-Validation (why & how we do it): 2020-09-15\n",
        "- feature engineering\n",
        "  - one-hot-encoding (dummy variables)\n",
        "  - imputation schemes\n",
        "  - Box-Cox & Yeo Johnson transformation\n",
        "  - dimensionality reduction (PCA)\n",
        "\n",
        "\n",
        "### Specific Algorithms\n",
        "General questions that apply to all algorithms:\n",
        "  - give a high-level description and picture summarizing how it works (ex: decision boundary)\n",
        "  - describe when it can be used.  For example, what type(s) of features can it handle (cts/cat/mixed); do all features need to be scaled to mean 0 variance 1; must features must be approximately normal, etc?\n",
        "  - identify key hyperparameters, describe the meaning/significance/role/effect when you change it\n",
        "  - answer specific questions below\n",
        "\n",
        "Specific questions\n",
        "1. Principal Components\n",
        "  - What is a principle component?   What is the princple component transformation?  How do you get it and how do you use it?\n",
        "  - How do you compute the variance explained by the first $k$ principal components?\n",
        "\n",
        "1. k-Nearest Neighbors\n",
        "  - What happens as $k$ varies?\n",
        "\n",
        "1. Naive Bayes Classifier\n",
        "  - Name at least 2 types of NB and the situation where they apply\n",
        "  - We went deep into the situation with all categorical variables.  For this situation:\n",
        "    - State Bayes Theorem\n",
        "    - Derive the \"not-naive\" Bayes Classifier equation and explain the problems that prevent us from using it.\n",
        "    - State the \"naive\" assumption and use it to turn the \"not-naive\" Bayes expression above into the naive Bayes expression we actually use.\n",
        "\n",
        "1. Decision Trees\n",
        "  - How do they work\n",
        "  - State 2 different common impurity measures and the properties they have in common\n",
        "  - Why are they so prone to overfitting?  Name some ways to combat this problem\n",
        "  - Recall there are many implementations (CART, ID3, C4.5, etc) with different rules for growing decision trees.  You are NOT expected to describe those rules.\n",
        "  - Why is it bad to use 1 decision tree by itself?  What is the improved version called and why is it better (in general terms)?\n",
        "\n",
        "1. Support Vector Machines\n",
        "  - Totally Linearly Separable case\n",
        "    - What are $\\vec{\\beta}$ and $\\vec{z}$.\n",
        "    - State the \"original\" optimization problems (OPT1 in notes) and explain what each part means.\n",
        "    - Names associated to the \"conditions\" that let us convert OPT3 into OPT4\n",
        "    - Equation e was critical because it split rows into 2 cases.  Describe these 2 cases and why the first case dramatically simplifies the problem.\n",
        "  - ~~Totally~~ Linearly Separable case\n",
        "    - State the new optimization problem and describe the new variables (slack and cost)\n",
        "  - ~~Totally~~ ~~Linearly~~ Separable case\n",
        "    - What is $\\vec{h}(\\vec{x})$?\n",
        "    - State the big idea (3 steps)\n",
        "    - \"This big idea is typically too computationally expensive.  But, for some special choices of $\\vec{h}(\\vec{x})$, we can use _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ to simplify the calculations significantly.\"  Fill in the blank and describe what it is."
      ]
    }
  ]
}